{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# set up imports\n",
    "import pandas as pd, numpy as np, seaborn as sns\n",
    "from tabulate import tabulate\n",
    "import statsmodels\n",
    "import arch\n",
    "import matplotlib\n",
    "matplotlib.use('qt5agg')\n",
    "\n",
    "# configure plot style\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['mathtext.fontset'] = 'stix'\n",
    "plt.rcParams['font.family'] = 'STIXGeneral'\n",
    "plt.rcParams[\"figure.figsize\"] = (9.5,4.15)\n",
    "plt.rcParams['figure.constrained_layout.use'] = False\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['savefig.dpi'] = 10\n",
    "plt.rcParams['lines.linewidth'] = 0.8\n",
    "save_plot_to =  r'C:\\\\Users\\joche\\OneDrive\\03 TUM - TUM-BWL\\Semester 8\\01 Bachelorarbeit\\04 Results\\Plots/'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# load data and crop to start_date:end_date\n",
    "start_date = '2015-08-07'\n",
    "end_date = '2020-06-26'\n",
    "idx = pd.date_range(start_date, end_date)\n",
    "index_name = 'date'\n",
    "mydateparser = lambda x: pd.datetime.strptime(str(x), '%Y-%m-%d %H:%M:%S')\n",
    "mydateparser1 = lambda x: pd.datetime.strptime(str(x), '%Y-%m-%d')\n",
    "\n",
    "# btc\n",
    "btc = pd.read_excel('Data/BTC_closing.xlsx', parse_dates=[0], index_col=0, date_parser=mydateparser)[start_date:end_date]\n",
    "btc.index.name = index_name\n",
    "btc.columns = ['btc']\n",
    "\n",
    "# usd_eur\n",
    "usd_eur = pd.read_excel('Data/DEXUSEU.xls', parse_dates=[0], index_col=0, skiprows=10, date_parser=mydateparser)[start_date:end_date]\n",
    "usd_eur.index.name = index_name\n",
    "usd_eur.columns = ['usd_eur']\n",
    "usd_eur = usd_eur.loc[(usd_eur!=0).any(1)]\n",
    "\n",
    "# tot_btc: only weekly data - missing values interpolated\n",
    "tot_btc = pd.read_csv('Data/total-bitcoins.txt', index_col=0)[start_date:end_date]\n",
    "tot_btc.index = pd.DatetimeIndex(tot_btc.index, normalize=True).normalize()\n",
    "tot_btc.index.name = index_name\n",
    "tot_btc = tot_btc.reindex(idx, fill_value=None)\n",
    "tot_btc.interpolate(method='time', inplace=True, limit_direction='both')\n",
    "tot_btc.columns = ['tot_btc']\n",
    "\n",
    "# hs_rate: only weekly data - missing values interpolated\n",
    "hs_rate = pd.read_csv('Data/hash-rate.txt', index_col=0)[start_date:end_date]\n",
    "hs_rate.index = pd.DatetimeIndex(hs_rate.index, normalize=True).normalize()\n",
    "hs_rate.index.name = index_name\n",
    "hs_rate = hs_rate.reindex(idx, fill_value=None)\n",
    "hs_rate.interpolate(method='time', inplace=True, limit_direction='both')\n",
    "hs_rate.columns = ['hs_rate']\n",
    "\n",
    "# eth\n",
    "eth = pd.read_excel('Data/ETH.xlsx', parse_dates=[0], index_col=0, date_parser=mydateparser, usecols='A,E')[start_date:end_date]\n",
    "eth.index.name = index_name\n",
    "eth.columns = ['eth']\n",
    "\n",
    "# ggl_trends: only weekly data - missing values interpolated\n",
    "# ggl_trends: weighted average of 15 countries\n",
    "ggl_trends = pd.read_csv('Data/googletrends.txt', index_col=0)[start_date:end_date]\n",
    "ggl_trends.index = pd.DatetimeIndex(ggl_trends.index, normalize=True).normalize()\n",
    "ggl_trends.index.name = index_name\n",
    "ggl_trends = ggl_trends.reindex(idx, fill_value=None)\n",
    "ggl_trends.interpolate(method='time', inplace=True, limit_direction='both')\n",
    "ggl_trends['btc-average'] = np.round(ggl_trends.sum(axis=1)/15)\n",
    "ggl_trends = ggl_trends[['btc-average']].copy()\n",
    "ggl_trends.columns = ['ggl_trends']\n",
    "\n",
    "# wiki_views: sum of 99 countries\n",
    "wiki_views = pd.read_excel('Data/wikipedia.xlsx', parse_dates=[0], index_col=0, date_parser=mydateparser1)[start_date:end_date]\n",
    "wiki_views.index = pd.DatetimeIndex(wiki_views.index, normalize=True).normalize()\n",
    "wiki_views.index.name = index_name\n",
    "wiki_views['wiki-total'] = np.round(wiki_views.sum(axis=1))\n",
    "wiki_views = wiki_views[['wiki-total']].copy()\n",
    "wiki_views.columns = ['wiki_views']\n",
    "\n",
    "# wti_oil\n",
    "oil_wti = pd.read_excel('Data/DCOILWTICO.xls', parse_dates=[0], index_col=0, skiprows=10, date_parser=mydateparser)[start_date:end_date]\n",
    "oil_wti.index = pd.DatetimeIndex(oil_wti.index, normalize=True).normalize()\n",
    "oil_wti.index.name = index_name\n",
    "oil_wti.columns = ['oil_wti']\n",
    "oil_wti = oil_wti.loc[(oil_wti!=0).any(1)]\n",
    "\n",
    "# gold\n",
    "gold = pd.read_excel('Data/GOLDAMGBD228NLBM10AM.xls', parse_dates=[0], index_col=0, skiprows=10, date_parser=mydateparser)[start_date:end_date]\n",
    "gold.index = pd.DatetimeIndex(gold.index, normalize=True).normalize()\n",
    "gold.index.name = index_name\n",
    "gold.columns = ['gold']\n",
    "gold = gold.loc[(gold!=0).any(1)]\n",
    "\n",
    "# sp500\n",
    "sp500 = pd.read_excel('Data/SP500.xls', parse_dates=[0], index_col=0, skiprows=10, date_parser=mydateparser)[start_date:end_date]\n",
    "sp500.index = pd.DatetimeIndex(sp500.index, normalize=True).normalize()\n",
    "sp500.index.name = index_name\n",
    "sp500.columns = ['sp500']\n",
    "sp500 = sp500.loc[(sp500!=0).any(1)]\n",
    "\n",
    "# sse \n",
    "sse = pd.read_excel('Data/SSEcomposite.xlsx', parse_dates=[0], index_col=0, header=0, date_parser=mydateparser)[start_date:end_date]\n",
    "sse.index = pd.DatetimeIndex(sse.index, normalize=True).normalize()\n",
    "sse.index.name = index_name\n",
    "sse = sse[['Zuletzt']].copy()\n",
    "sse.columns = ['sse']\n",
    "\n",
    "# ffd_rate\n",
    "ffd_rate = pd.read_excel('Data/DFF.xls', parse_dates=[0], index_col=0, skiprows=10, date_parser=mydateparser)[start_date:end_date]\n",
    "ffd_rate.index = pd.DatetimeIndex(ffd_rate.index, normalize=True).normalize()\n",
    "ffd_rate.index.name = index_name\n",
    "ffd_rate.columns = ['ffd_rate']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# merge data to one df (inner join)\n",
    "from functools import reduce\n",
    "temp = [btc,tot_btc,hs_rate,eth,ggl_trends,wiki_views,usd_eur,oil_wti,\n",
    "              gold,sp500,sse,ffd_rate]\n",
    "df = reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True), temp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# split data in pre and post bubble 2018\n",
    "pre_bubble_end_date = '2017-12-01'\n",
    "post_bubble_start_date = '2018-02-01'\n",
    "\n",
    "# pre bubble df\n",
    "pre_df = df[:pre_bubble_end_date]\n",
    "\n",
    "# post bubble df\n",
    "post_df = df[post_bubble_start_date:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# save btc_new.txt dataset with 1112 observations for use in M1-M3\n",
    "def save_btcnew():\n",
    "    btc_cropped = df['btc']\n",
    "    btc_cropped.to_csv(path_or_buf=save_plot_to+'btc_new.txt')\n",
    "\n",
    "# save_btcnew()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-25c253ae4b1a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m11\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;31m# tests for cointegration at i lags\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 335\u001b[1;33m     \u001b[0mjoh_trace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjohansen_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_select_train_log\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    336\u001b[0m     \u001b[1;31m# prints the results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m     print('Using the Trace Test, there are', joh_trace.r, '''cointegrating vectors at \n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_select_train_log' is not defined"
     ],
     "ename": "NameError",
     "evalue": "name 'df_select_train_log' is not defined",
     "output_type": "error"
    }
   ],
   "source": [
    "### RUN TO DEFINE ALL FUNCTIONS ###\n",
    "\n",
    "# define overview-printing function of dataframe\n",
    "def dates_overview(dataframe):\n",
    "    print('start_date:\\t', dataframe.index[0])\n",
    "    print('end_date:\\t', dataframe.index[-1])\n",
    "    print('len df:\\t\\t', len(dataframe))\n",
    "    \n",
    "# define plotting function of dataframe\n",
    "import matplotlib.dates as mdates\n",
    "def plot_df(dataframe):\n",
    "    fig_i, axs = plt.subplots(4,3, figsize=(9.5,4.15))\n",
    "    for i, ax in enumerate(axs.flatten()):\n",
    "        data = dataframe[dataframe.columns[i]]\n",
    "        ax.plot(data, color='black')\n",
    "        ax.set_title(dataframe.columns[i])\n",
    "        ax.xaxis.set_ticks_position('none')\n",
    "        ax.yaxis.set_ticks_position('none')\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m\"))\n",
    "        ax.xaxis.set_minor_formatter(mdates.DateFormatter(\"%Y-%m\"))\n",
    "        #ax.spines['top'].set_alpha(0)\n",
    "        ax.tick_params(labelsize=8)\n",
    "    fig_i.set_size_inches(12,5)\n",
    "    fig_i.tight_layout()\n",
    "    return fig_i\n",
    "\n",
    "# define log-taking and relabeling function\n",
    "def log_of_df(dataframe):\n",
    "    df_log = np.log(dataframe)\n",
    "    new_cols = list()\n",
    "    for i in df_log.columns:\n",
    "        new_cols.append(i+'_log')\n",
    "    df_log.columns = new_cols\n",
    "    # fill na value of negative oil price on 2020-04-20 with 0\n",
    "    df_log.fillna(value=0, inplace=True)\n",
    "    return df_log\n",
    "\n",
    "# define correlation-table generating function of dataframe\n",
    "def corr_table_aslatex_of_df(dataframe):\n",
    "    corr = dataframe.corr().round(3)\n",
    "    print(tabulate(corr, headers=corr.columns, showindex=True, tablefmt=\"latex\"))\n",
    "\n",
    "# define heatmap-generating function of dataframe\n",
    "def heatmap_corr_of_df(dataframe):\n",
    "    corr = dataframe.corr()\n",
    "    # more html colors here: https://www.w3schools.com/colors/colors_names.asp\n",
    "    # pal = sns.light_palette('lightgrey', as_cmap=True)\n",
    "    ax = sns.heatmap(corr, xticklabels=corr.columns.values, yticklabels=corr.columns.values, annot=True, \n",
    "            annot_kws={'size':7}, vmin=-1, center=0, vmax=1, cmap=\"YlGnBu\")\n",
    "    bottom, top = ax.get_ylim()\n",
    "    ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "    fig_3 = plt.gcf()\n",
    "    fig_3.set_size_inches(9.5,4.15)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=7.5, labelbottom = False, bottom=False, top = False, labeltop=True)\n",
    "    plt.xticks(fontsize=7.5, rotation=1)\n",
    "    plt.yticks(fontsize=7.5)\n",
    "    \n",
    "# define train/test-splitting function of dataframe\n",
    "def split_traintest_df(dataframe):\n",
    "    train_size = int(len(dataframe) * 0.8)\n",
    "    df_train, df_test = dataframe[0:train_size], dataframe[train_size:]\n",
    "    # create dataframe for printout\n",
    "    data = {'Dataframe': ['dataframe', 'df_train', 'df_test'],\n",
    "            'date_start': [dataframe.index[0], df_train.index[0], df_test.index[0]],\n",
    "            'date_end': [dataframe.index[-1], df_train.index[-1], df_test.index[-1]],\n",
    "            'nobs': [len(dataframe), len(df_train), len(df_test)]}\n",
    "    df_print = pd.DataFrame(data, columns=['Dataframe','date_start','date_end','nobs'])\n",
    "    print(df_print)\n",
    "    return df_train, df_test\n",
    "\n",
    "# define adf- and pp-testing of dataframe with latex-printout on/off\n",
    "from arch.unitroot import ADF, PhillipsPerron\n",
    "def stationarity_tests(dataframe, latex):\n",
    "    for col in dataframe:\n",
    "        adf = ADF(dataframe[col])\n",
    "        pp = PhillipsPerron(dataframe[col])\n",
    "        if latex is False:\n",
    "            # write summary as plain text to std.out\n",
    "            print('Timeseries:\\t',col,'\\n',\n",
    "                  adf.summary(),'\\n\\n',pp.summary(),'\\n\\n\\n')\n",
    "        else:\n",
    "            # write summary as latex to file\n",
    "            with open(save_plot_to + 'Stationarity_Tests_LaTeX.txt', 'a') as myfile:\n",
    "                myfile.write('Timeseries:\\t'+col+'\\n'\n",
    "                         +adf.summary().as_latex()+'\\n\\n'\n",
    "                         +pp.summary().as_latex()+'\\n\\n\\n')\n",
    "\n",
    "# define first difference-taking function of dataframe\n",
    "def diff_of_df(dataframe):\n",
    "    df_train_log_diff = df_train_log.diff()\n",
    "    # relabel columns\n",
    "    new_cols = list()\n",
    "    for i in df_train_log_diff.columns:\n",
    "        new_cols.append(i+'_diff')\n",
    "    df_train_log_diff.columns = new_cols\n",
    "    return df_train_log_diff\n",
    "\n",
    "# define stationaritiy table-generating function of a dataframe filepath\n",
    "def stationarity_table_aslatex_from_df(filepath):\n",
    "    # read in dataframe from .txt file\n",
    "    stationarity_dataframe = pd.read_csv(save_plot_to+filepath, delimiter='\\s+', header=0)\n",
    "    # print dataframe as latex output\n",
    "    print(tabulate(stationarity_dataframe, headers=stationarity_dataframe.columns, showindex=False, tablefmt=\"latex\"))\n",
    "\n",
    "# define granger causality test performing function of a dataframe\n",
    "# code taken from: https://stackoverflow.com/questions/58005681/is-it-possible-to-run-a-vector-autoregression-analysis-on-a-large-gdp-data-with\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "maxlag=15\n",
    "test = 'ssr-chi2test'\n",
    "def grangers_causality_matrix(X_train, variables, test = 'ssr_chi2test', verbose=False):\n",
    "    dataset = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n",
    "    for c in dataset.columns:\n",
    "        for r in dataset.index:\n",
    "            test_result = grangercausalitytests(X_train[[r,c]], maxlag=maxlag, verbose=False)\n",
    "            p_values = [round(test_result[i+1][0][test][1],4) for i in range(maxlag)]\n",
    "            if verbose: print(f'Y = {r}, X = {c}, P Values = {p_values}')\n",
    "            min_p_value = np.min(p_values)\n",
    "            dataset.loc[r,c] = min_p_value\n",
    "    dataset.columns = [var + '_x' for var in variables]\n",
    "    dataset.index = [var + '_y' for var in variables]\n",
    "    return dataset\n",
    "\n",
    "# define granger causality table generating function from dataframe\n",
    "def grangercausality_table_aslatex_of_df(dataframe):\n",
    "    print(tabulate(dataframe, headers=dataframe.columns, showindex=True, tablefmt=\"latex\"))\n",
    "\n",
    "# define grangercausality heatmap generating function of a dataframe\n",
    "def grangercausality_heatmap_of_df(dataframe):\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.heatmap(dataframe, xticklabels=dataframe.columns.values, yticklabels=dataframe.index.values,\n",
    "                 annot=True, annot_kws={'size':7}, vmin=0, vmax=1, cmap=\"YlGnBu_r\", ax=ax)\n",
    "    bottom, top = ax.get_ylim()\n",
    "    ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(9.5,4.15)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=7, labelbottom = False, bottom=False, top = False, labeltop=True)\n",
    "    plt.xticks(fontsize=7, rotation=45)\n",
    "    plt.yticks(fontsize=7)\n",
    "    plt.set_size_inches(12,5)\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# define VAR-order selecting function of dataframe\n",
    "from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "def select_VAR_order(dataframe):\n",
    "    var_model = VAR(endog=dataframe)\n",
    "    var_order_res = var_model.select_order(15).summary()\n",
    "    res_as_html = var_order_res.as_html()\n",
    "    df_var_order = pd.read_html(res_as_html, header=0, index_col=0)[0]\n",
    "    return df_var_order\n",
    "\n",
    "# define VAR-order latex-table-generating function of a var_order\n",
    "def var_order_aslatex_of_order(var_order):\n",
    "    print(tabulate(var_order, headers=var_order.columns, showindex=True, tablefmt=\"latex\"))\n",
    "\n",
    "# define VAR-model-fitting function of a dataframe and a lag order\n",
    "def fit_VAR_model_of_df(dataframe, order):\n",
    "    var_model = VAR(endog=dataframe)\n",
    "    var_fitted = var_model.fit(maxlags = order)\n",
    "    return var_fitted\n",
    "\n",
    "\n",
    "# transform var_fitted into data frame\n",
    "# code taken from: https://stackoverflow.com/questions/51734180/converting-statsmodels-summary-object-to-pandas-dataframe\n",
    "def results_summary_to_dataframe(results):\n",
    "    '''take the result of an statsmodel results table and transforms it into a dataframe'''\n",
    "    pvals = results.pvalues\n",
    "    coeff = results.params\n",
    "    conf_lower = results.conf_int()[0]\n",
    "    conf_higher = results.conf_int()[1]\n",
    "\n",
    "    results_df = pd.DataFrame({\"pvals\":pvals,\n",
    "                               \"coeff\":coeff,\n",
    "                               \"conf_lower\":conf_lower,\n",
    "                               \"conf_higher\":conf_higher\n",
    "                                })\n",
    "    #Reordering\n",
    "    results_df = results_df[[\"coeff\",\"pvals\",\"conf_lower\",\"conf_higher\"]]\n",
    "    return results_df\n",
    "\n",
    "# define dataframe generating function from .txt file at filepath\n",
    "def var_coeffs_aslatex_from_txt(filepath):\n",
    "    # read in df from textfile\n",
    "    var_btc_coefficients = pd.read_csv(save_plot_to+filepath, delimiter='\\s+', header=0)\n",
    "    var_btc_coefficients.set_index(var_btc_coefficients.columns[0], inplace=True)\n",
    "    # generate latex output\n",
    "    print(tabulate(var_btc_coefficients.round(4), headers=var_btc_coefficients.columns, showindex=True, tablefmt=\"latex\"))\n",
    "\n",
    "# define rolling forecast function\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "def var_rolling_forecast(dataframe, lag_order):\n",
    "    # create train and test dataframes\n",
    "    train_size = int(len(dataframe) * 0.8)\n",
    "    dataframe_train, dataframe_test = dataframe[0:train_size], dataframe[train_size:]\n",
    "    index = len(dataframe_train) - 1\n",
    "    # initialize lists\n",
    "    pred_val_btc = list()\n",
    "    lo_conf_int_btc = list()\n",
    "    up_conf_int_btc = list()\n",
    "    ausreisser_ctr = 0\n",
    "    # rolling forecast\n",
    "    for i in range(len(dataframe_test)):\n",
    "        # input data, dependent on i\n",
    "        input_data = log_of_df(dataframe).diff().values[lag_order+1:index + i]\n",
    "        # fit model and predict 1 step\n",
    "        dataframe_var = VAR(endog=input_data)\n",
    "        dataframe_var_fitted = dataframe_var.fit(maxlags=lag_order)\n",
    "        dataframe_var_result = dataframe_var_fitted.forecast_interval(y=input_data, steps=1, alpha=0.05)\n",
    "        # obtain absolute (inversed) btc values for mean prediction, upper- and lower confidence interval\n",
    "        yhat_btc = np.exp(dataframe_var_result[0][0][0] + np.log(dataframe).iloc[:,0][index+i])\n",
    "        lo_conf_btc_val = np.exp(dataframe_var_result[1][0][0] + np.log(dataframe).iloc[:,0][index+i])\n",
    "        up_conf_btc_val = np.exp(dataframe_var_result[2][0][0] + np.log(dataframe).iloc[:,0][index+i])\n",
    "        if (dataframe.iloc[:,0][index+i+1] > up_conf_btc_val) or (dataframe.iloc[:,0][index+i+1] < lo_conf_btc_val):\n",
    "            ausreisser_ctr += 1\n",
    "        pred_val_btc.append(yhat_btc)\n",
    "        lo_conf_int_btc.append(lo_conf_btc_val)\n",
    "        up_conf_int_btc.append(up_conf_btc_val)\n",
    "    # return [0]: mse, [1]: ausreisser ctr, [2]: predictions, [3]: lo_confint, [4]: up_confint\n",
    "    return (np.sqrt(mse(pred_val_btc, dataframe_test.iloc[:,0].values)), ausreisser_ctr,\n",
    "            pred_val_btc, lo_conf_int_btc, up_conf_int_btc)\n",
    "\n",
    "# define series-generating and plotting function for dataframe_var_pred_result\n",
    "def plot_var_pred_result(dataframe, dataframe_train, dataframe_test, dataframe_var_pred_result):\n",
    "    # make series for plotting pred. vs. actual\n",
    "    index_pred = np.arange(len(dataframe_train) + 1, len(dataframe) + 1)\n",
    "    pred_val_btc_series = pd.Series(dataframe_var_pred_result[2], index=index_pred)\n",
    "    test_series_btc = pd.Series(dataframe_test.iloc[:,0].values, index=index_pred)\n",
    "    lo_conf_int_btc_series = pd.Series(dataframe_var_pred_result[3], index=index_pred)\n",
    "    up_conf_int_btc_series = pd.Series(dataframe_var_pred_result[4], index=index_pred)\n",
    "    # create plot: \n",
    "    fig, axs = plt.subplots(1,2)\n",
    "    axs[0].plot(dataframe_train.iloc[:,0].values, label= r'$btc_T$', color='black')\n",
    "    axs[0].plot(test_series_btc, label= r'$btc_{T+h}$', color='green')\n",
    "    axs[0].plot(pred_val_btc_series, label= r'$\\hat{btc}_{T+h}$', color= 'red')\n",
    "    axs[0].fill_between(lo_conf_int_btc_series.index, lo_conf_int_btc_series, up_conf_int_btc_series, color='k', alpha=0.1)\n",
    "    axs[0].legend(loc='upper left')\n",
    "    axs[0].title.set_text('Gesamter Zeitraum')\n",
    "    axs[1].plot(test_series_btc, label= r'$btc_{T+h}$', color='green')\n",
    "    axs[1].plot(pred_val_btc_series, label= r'$\\hat{btc}_{T+h}$', color= 'red')\n",
    "    axs[1].fill_between(lo_conf_int_btc_series.index, lo_conf_int_btc_series, up_conf_int_btc_series, color='k', alpha=0.1)\n",
    "    axs[1].legend(loc='upper left')\n",
    "    axs[1].title.set_text('Vorhersage-Zeitraum')\n",
    "    fig.set_size_inches(15,2.5)\n",
    "    return fig\n",
    "\n",
    "# define cointegration rank testing function based on select_coint_rank of a dataframe\n",
    "from statsmodels.tsa.vector_ar.vecm import select_coint_rank\n",
    "def print_select_coint_rank(dataframe, method, k_ar_diff, signif):\n",
    "    dataframe_vecm_rank = select_coint_rank(dataframe,det_order=-1,k_ar_diff=k_ar_diff,method=method,signif=signif)\n",
    "    print(dataframe_vecm_rank.summary())\n",
    "    # summary output must be copied and saved as .txt\n",
    "\n",
    "# define trace cointegration testing function for dataframe\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "def print_cointegration_test_of(dataframe, k_ar_diff, signif):\n",
    "    result = coint_johansen(dataframe, -1, k_ar_diff)\n",
    "    d = {'0.90':0, '0.95':1, '0.99':2}\n",
    "    trace_stat = result.lr1\n",
    "    crit_val_trace_stat = result.cvt[:, d[str(1-signif)]]\n",
    "    \n",
    "    def adjust(val, length = 6): \n",
    "        return str(val).ljust(length)\n",
    "    \n",
    "    print(' Variable  \\t\\ttest-statistic\\tCV(95%)\\t\\tSignif\\n')\n",
    "    for col, trace, cvt in zip(dataframe.columns, trace_stat, crit_val_trace_stat):\n",
    "        if col is 'dax' or col is 'sp500':\n",
    "            tab = 2\n",
    "        else:\n",
    "            tab = 1\n",
    "        print(adjust(col), '\\t'*tab, adjust(round(trace,2), 9),'\\t', adjust(cvt, 8), '\\t' , trace > cvt)\n",
    "    # -> Copy summary output and save as .txt\n",
    "    \n",
    "# define VECM specifying function for dataframe\n",
    "from statsmodels.tsa.vector_ar.vecm import VECM\n",
    "def get_vecm_model(dataframe,k_ar_diff,coint_rank,det):\n",
    "    dataframe_vecm = VECM(endog=dataframe,k_ar_diff=k_ar_diff,coint_rank=coint_rank,deterministic=det)\n",
    "    dataframe_vecm_fitted = dataframe_vecm.fit()\n",
    "    return dataframe_vecm_fitted\n",
    "\n",
    "# define rolling forecast function\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "def vecm_rolling_forecast(dataframe, lag_order, coint_rank, det):\n",
    "    # create train and test dataframes\n",
    "    train_size = int(len(dataframe) * 0.8)\n",
    "    dataframe_train, dataframe_test = dataframe[0:train_size], dataframe[train_size:]\n",
    "    index = len(dataframe_train) - 1\n",
    "    # initialize lists\n",
    "    pred_val_btc = list()\n",
    "    lo_conf_int_btc = list()\n",
    "    up_conf_int_btc = list()\n",
    "    ausreisser_ctr = 0\n",
    "    # rolling forecast\n",
    "    for i in range(len(dataframe_test)):\n",
    "        # input data, dependent on i\n",
    "        input_data = log_of_df(dataframe).values[lag_order:index + i]\n",
    "        # fit model and predict 1 step\n",
    "        dataframe_vecm_fitted = get_vecm_model(dataframe=input_data,k_ar_diff=lag_order,coint_rank=coint_rank,det=det)\n",
    "        dataframe_vecm_result = dataframe_vecm_fitted.predict(steps=1,alpha=0.05)\n",
    "        # obtain absolute (inversed) btc values for mean prediction, upper- and lower confidence interval\n",
    "        yhat_btc = np.exp(dataframe_vecm_result[0][0][0])\n",
    "        lo_conf_btc_val = np.exp(dataframe_vecm_result[1][0][0])\n",
    "        up_conf_btc_val = np.exp(dataframe_vecm_result[2][0][0])\n",
    "        if (dataframe.iloc[:,0][index+i+1] > up_conf_btc_val) or (dataframe.iloc[:,0][index+i+1] < lo_conf_btc_val):\n",
    "            ausreisser_ctr += 1\n",
    "        pred_val_btc.append(yhat_btc)\n",
    "        lo_conf_int_btc.append(lo_conf_btc_val)\n",
    "        up_conf_int_btc.append(up_conf_btc_val)\n",
    "    # return [0]: mse, [1]: ausreisser ctr, [2]: predictions, [3]: lo_confint, [4]: up_confint\n",
    "    return (np.sqrt(mse(pred_val_btc, dataframe_test.iloc[:,0].values)), ausreisser_ctr,\n",
    "            pred_val_btc, lo_conf_int_btc, up_conf_int_btc)\n",
    "\n",
    "\n",
    "## not used ##\n",
    "# imports the coint_johansen function to test for cointegration as a prerequisite for VEC \n",
    "# modeling\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "\n",
    "# uses https://nbviewer.jupyter.org/github/mapsa/seminario-doc-2014/blob/master/cointegration-\n",
    "# example.ipynb to create functions to return the number of cointegrating vectors based \n",
    "# on the Trace version if the Johansen Cointegration Test\n",
    "def johansen_trace(y, p):\n",
    "        N, l = y.shape\n",
    "        joh_trace = coint_johansen(y, 0, p)\n",
    "        r = 0\n",
    "        for i in range(l):\n",
    "            if joh_trace.lr1[i] > joh_trace.cvt[i, 1]:     # 0: 90%  1:95% 2: 99%\n",
    "                r = i + 1\n",
    "        joh_trace.r = r\n",
    "\n",
    "        return joh_trace\n",
    "\n",
    "# loops through 1 to 10 lags of trading days\n",
    "for i in range(1, 11): \n",
    "    # tests for cointegration at i lags\n",
    "    joh_trace = johansen_trace(df_select_train_log, i)\n",
    "    # prints the results\n",
    "    print('Using the Trace Test, there are', joh_trace.r, '''cointegrating vectors at \n",
    "    %s lags between the df_select_train_log''' % i)\n",
    "    # prints a space for readability\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# overview of df\n",
    "dates_overview(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# plot df\n",
    "fig_1 = plot_df(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# take log of df\n",
    "df_log = log_of_df(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# plot log time series\n",
    "fig_2 = plot_df(df_log)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# save fig_2\n",
    "fig_2.savefig(save_plot_to+'M4_fig_2.svg',format='svg',bbox_inches='tight',pad_inches = 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# correlation matrix of df\n",
    "corr_table_aslatex_of_df(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# correlation heatmap of df\n",
    "heatmap_corr_of_df(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# split df in df_train and df_test\n",
    "df_train, df_test = split_traintest_df(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "# take log of df_train\n",
    "df_train_log = log_of_df(df_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "# plot df_train_log\n",
    "fig_4 = plot_df(df_train_log)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "# adf- and pp-testing of df_train_log - no latex printout\n",
    "stationarity_tests(df_train_log, latex=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# take first difference of df_train_log\n",
    "df_train_log_diff = diff_of_df(df_train_log)\n",
    "df_train_log_diff"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# plot df_train_log_diff\n",
    "fig_5 = plot_df(df_train_log_diff[1:])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# save fig_5\n",
    "fig_5.savefig(save_plot_to+'M4_fig_5.svg',format='svg',bbox_inches='tight',pad_inches = 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "# adf- and pp-testing of df_train_log_diff - no latex printout\n",
    "stationarity_tests(df_train_log_diff[1:], latex=False)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "# plot stationarity table of df\n",
    "stationarity_table_aslatex_from_df('M4_stationarity tests.txt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# perform granger causality test each on each based on df_train_log_diff\n",
    "granger_causality_matrix = grangers_causality_matrix(df_train_log_diff[1:], variables = df_train_log_diff.columns)\n",
    "granger_causality_matrix.round(4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "# generate granger causality table as latex output\n",
    "grangercausality_table_aslatex_of_df(granger_causality_matrix)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# generate granger causality heatmap of df_train_log_diff\n",
    "fig_6 = grangercausality_heatmap_of_df(granger_causality_matrix)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "# save fig_6\n",
    "fig_6.savefig(save_plot_to+'M4_fig_6.svg',format='svg',bbox_inches='tight',pad_inches = 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## VAR: df_select_train_log_diff (3 Variables) ##"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "# based on granger-causality: new df with btc, hs_rate and ggl_trends\n",
    "df_select_train_log_diff = df_train_log_diff[['btc_log_diff','hs_rate_log_diff','ggl_trends_log_diff']]\n",
    "df_select_train_log_diff"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "# select VAR-order for df_select_train_log_diff: 5\n",
    "df_select_var_order = select_VAR_order(df_select_train_log_diff[1:])\n",
    "df_select_var_order"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "# generate var_select_order latex output from df_select_var_order\n",
    "var_order_aslatex_of_order(df_select_var_order)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "# obtain VAR-model for df_select_train_log_diff and lag order = 5\n",
    "df_select_var = fit_VAR_model_of_df(df_select_train_log_diff[1:], 5)\n",
    "df_select_var.summary()\n",
    "# -> Copy summary output and save as .txt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "# read in and plot var_select_coeffs from textfile as latex: 'M4_df_select_VAR_coefficients.txt'\n",
    "var_coeffs_aslatex_from_txt('M4_df_select_VAR_coefficients.txt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# rolling forecast: obtain mse, ausreisser, predictions, lo-& up confint for df_select with lag order = 5\n",
    "df_select_var_pred_result = var_rolling_forecast(df[['btc','hs_rate','ggl_trends']], 5)\n",
    "\n",
    "# printout results\n",
    "print('mse_select:\\t\\t', df_select_var_pred_result[0])\n",
    "print('ausreisser_select:\\t', df_select_var_pred_result[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "# rolling forecast: create series and plot them for df_var_pred_result\n",
    "fig_7 = plot_var_pred_result(df[['btc','hs_rate','ggl_trends']],\n",
    "                             df_train[['btc','hs_rate','ggl_trends']],\n",
    "                             df_test[['btc','hs_rate','ggl_trends']],\n",
    "                             df_select_var_pred_result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "# save fig_7\n",
    "fig_7.savefig(save_plot_to+'M4_fig_7.svg',format='svg',bbox_inches='tight',pad_inches = 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## VAR: df_train_log_diff (12 Variables) ##"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "# select VAR-order for df_train_log_diff: 5\n",
    "df_var_order = select_VAR_order(df_train_log_diff[1:])\n",
    "df_var_order"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# generate var_order latex output from df_var_order\n",
    "var_order_aslatex_of_order(df_var_order)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# obtain VAR-model for df_train_log_diff and lag order = 5\n",
    "df_var = fit_VAR_model_of_df(df_train_log_diff[1:], 5)\n",
    "df_var.summary()\n",
    "# -> Copy summary output and save as .txt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# read in and plot var_coeffs from textfile: 'M4_df_VAR_coefficients.txt'\n",
    "var_coeffs_aslatex_from_txt('M4_df_VAR_coefficients.txt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# IRF: impulse response of df_var\n",
    "from statsmodels.tsa.vector_ar.irf import IRAnalysis\n",
    "df_var_irf = df_var.irf(15)\n",
    "fig_9 = df_var_irf.plot(orth=False, response='btc_log_diff', figsize=(8,15),subplot_params={'fontsize' : 9})\n",
    "fig_9.tight_layout()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# save fig_9\n",
    "fig_9.savefig(save_plot_to+'M4_fig_9.svg',format='svg',bbox_inches='tight',pad_inches = 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "# IRF: cumulative impulse responses of df_var\n",
    "fig_10 = df_var_irf.plot_cum_effects(orth=False, response='btc_log_diff',figsize=(8,15),subplot_params={'fontsize' : 9})\n",
    "fig_10.tight_layout()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "# save fig_10\n",
    "fig_10.savefig(save_plot_to+'M4_fig_10.svg',format='svg',bbox_inches='tight',pad_inches = 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# rolling forecast: obtain mse, ausreisser, predictions, lo-& up confint for df with lag order = 5\n",
    "df_var_pred_result = var_rolling_forecast(df, 5)\n",
    "# printout results\n",
    "print('mse:\\t\\t', df_var_pred_result[0])\n",
    "print('ausreisser:\\t', df_var_pred_result[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# rolling forecast: create series and plot them for df_var_pred_result\n",
    "fig_8 = plot_var_pred_result(df,df_train,df_test,df_var_pred_result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# save fig_8\n",
    "fig_8.savefig(save_plot_to+'M4_fig_8.svg',format='svg',bbox_inches='tight',pad_inches = 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## VECM: df_select_train_log (3 Variables) ##"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "## ARCHIVE ##\n",
    "# series and lag plot to show trend and autocorrelation\n",
    "fig, axs = plt.subplots(2,1)\n",
    "axs[0].plot(df_log)\n",
    "pd.plotting.lag_plot(df_log, ax = axs[1])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "# based on granger-causality: new df with btc, hs_rate and ggl_trends\n",
    "df_select_train_log = df_train_log[['btc_log','hs_rate_log','ggl_trends_log']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "# determine cointegration rank of df_select_train_log with trace statistic\n",
    "print_select_coint_rank(df_select_train_log,'trace',10,0.05)\n",
    "# -> Copy summary output and save as .txt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "# read in and plot from textfile as latex: 'M4_select_tab_10_cointrank_trace.txt'\n",
    "var_coeffs_aslatex_from_txt('M4_select_tab_10_cointrank_trace.txt')\n",
    "# -> Copy latex output and input in overleaf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "# determine cointegration rank of df_select_train_log with maxeig statistic\n",
    "print_select_coint_rank(df_select_train_log,'maxeig',10,0.05)\n",
    "# -> Copy summary output and save as .txt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "# read in and plot from textfile as latex: 'M4_select_tab_11_cointrank_maxeig.txt'\n",
    "var_coeffs_aslatex_from_txt('M4_select_tab_11_cointrank_maxeig.txt')\n",
    "# -> Copy latex output and input in overleaf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "# print out johansen cointegration result for df_select_train_log\n",
    "print_cointegration_test_of(df_select_train_log,10,0.05)\n",
    "# -> Copy summary output and save as .txt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "# read in and plot from textfile as latex: 'M4_select_tab_12_johansen_cointrank_trace.txt'\n",
    "var_coeffs_aslatex_from_txt('M4_select_tab_12_johansen_cointrank_trace.txt')\n",
    "# -> Copy latex output and input in overleaf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "# obtain VECM(4) for df_select_train_log\n",
    "df_select_vecm = get_vecm_model(df_select_train_log,4,2,'nc')\n",
    "df_select_vecm.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "# rolling forecast: obtain mse, ausreisser, predictions, lo-& up confint for df_select with lag order = 4\n",
    "df_select_vecm_pred_result = vecm_rolling_forecast(df[['btc','hs_rate','ggl_trends']],lag_order=4,coint_rank=2,det='nc')\n",
    "\n",
    "# printout results\n",
    "print('mse_select:\\t\\t', df_select_vecm_pred_result[0])\n",
    "print('ausreisser_select:\\t', df_select_vecm_pred_result[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# rolling forecast: create series and plot them for df_vecm_pred_result\n",
    "fig_11 = plot_var_pred_result(df[['btc','hs_rate','ggl_trends']],\n",
    "                             df_train[['btc','hs_rate','ggl_trends']],\n",
    "                             df_test[['btc','hs_rate','ggl_trends']],\n",
    "                             df_select_vecm_pred_result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# save fig_11\n",
    "fig_11.savefig(save_plot_to+'M4_fig_11.svg',format='svg',bbox_inches='tight',pad_inches = 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## VECM: df_train_log (12 Variables) ##"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "# determine cointegration rank of df_train_log with trace statistic\n",
    "print_select_coint_rank(df_train_log,'trace',10,0.05)\n",
    "# -> Copy summary output and save as .txt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# read in and plot from textfile as latex: 'M4_tab_7_cointrank_trace.txt'\n",
    "var_coeffs_aslatex_from_txt('M4_tab_7_cointrank_trace.txt')\n",
    "# -> Copy latex output and input in overleaf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [],
   "source": [
    "# determine cointegration rank of df_train_log with maxeig statistic\n",
    "print_select_coint_rank(df_train_log,'maxeig',10,0.05)\n",
    "# -> Copy summary output and save as .txt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "# read in and plot from textfile as latex: 'M4_tab_8_cointrank_maxeig.txt'\n",
    "var_coeffs_aslatex_from_txt('M4_tab_8_cointrank_maxeig.txt')\n",
    "# -> Copy latex output and input in overleaf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [],
   "source": [
    "# print out johansen cointegration result for df_train_log\n",
    "print_cointegration_test_of(df_train_log,10,0.05)\n",
    "# -> Copy summary output and save as .txt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [],
   "source": [
    "# read in and plot from textfile as latex: 'M4_tab_9_johansen_cointrank_trace.txt'\n",
    "var_coeffs_aslatex_from_txt('M4_tab_9_johansen_cointrank_trace.txt')\n",
    "# -> Copy latex output and input in overleaf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "# obtain vecm(5-1) for df_train_log and coint_rank 6\n",
    "df_vecm = get_vecm_model(df_train_log,k_ar_diff=4,coint_rank=6,det='nc')\n",
    "df_vecm.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "# read in and plot from textfile as latex: 'M4_tab_13_vecm_coeffs.txt'\n",
    "var_coeffs_aslatex_from_txt('M4_tab_13_vecm_coeffs.txt')\n",
    "# -> Copy latex output and input in overleaf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "\\begin{tabular}{lrrrrrr}\n\\hline\n     &    coef &   std.err &      z &   P\\ensuremath{>}|z| &   [0.025 &   0.975] \\\\\n\\hline\n ec1 & -0.0379 &     0.011 & -3.577 &   0     &   -0.059 &   -0.017 \\\\\n ec2 & -0.0107 &     0.034 & -0.313 &   0.754 &   -0.077 &    0.056 \\\\\n ec3 &  0.0132 &     0.012 &  1.067 &   0.286 &   -0.011 &    0.038 \\\\\n ec4 &  0.0046 &     0.003 &  1.47  &   0.142 &   -0.002 &    0.011 \\\\\n ec5 &  0.0318 &     0.011 &  2.955 &   0.003 &    0.011 &    0.053 \\\\\n ec6 & -0.0061 &     0.009 & -0.69  &   0.49  &   -0.024 &    0.011 \\\\\n\\hline\n\\end{tabular}\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# read in and plot from textfile as latex: 'M4_tab_14_vecm_alpha1.txt'\n",
    "var_coeffs_aslatex_from_txt('M4_tab_14_vecm_alpha1.txt')\n",
    "# -> Copy latex output and input in overleaf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "\\begin{tabular}{lrrrrrr}\n\\hline\n         &     coef &   std.err &      z &   P\\ensuremath{>}|z| &   [0.025 &   0.975] \\\\\n\\hline\n beta.1  &   1      &     0     &  0     &   0     &    1     &    1     \\\\\n beta.2  &  -0      &     0     &  0     &   0     &   -0     &   -0     \\\\\n beta.3  &   0      &     0     &  0     &   0     &    0     &    0     \\\\\n beta.4  &  -0      &     0     &  0     &   0     &   -0     &   -0     \\\\\n beta.5  &   0      &     0     &  0     &   0     &    0     &    0     \\\\\n beta.6  &  -0      &     0     &  0     &   0     &   -0     &   -0     \\\\\n beta.7  & -16.0754 &     1.756 & -9.155 &   0     &  -19.517 &  -12.634 \\\\\n beta.8  &   1.894  &     0.628 &  3.015 &   0.003 &    0.663 &    3.125 \\\\\n beta.9  &   6.6093 &     2.634 &  2.509 &   0.012 &    1.447 &   11.772 \\\\\n beta.10 &  -7.8263 &     6.748 & -1.16  &   0.246 &  -21.052 &    5.399 \\\\\n beta.11 &  -0.1407 &     1.673 & -0.084 &   0.933 &   -3.42  &    3.139 \\\\\n beta.12 &   0.3692 &     4.524 &  0.082 &   0.935 &   -8.497 &    9.236 \\\\\n\\hline\n\\end{tabular}\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# read in and plot from textfile as latex: 'M4_tab_15_vecm_beta1.txt'\n",
    "var_coeffs_aslatex_from_txt('M4_tab_15_vecm_beta1.txt')\n",
    "# -> Copy latex output and input in overleaf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "# IRF: impulse response of df_vecm\n",
    "df_vecm_irf = df_vecm.irf(15)\n",
    "fig_12 = df_vecm_irf.plot(orth=False, response='btc_log', figsize=(8,15),subplot_params={'fontsize' : 9})\n",
    "fig_12.tight_layout()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "# save fig_12\n",
    "fig_12.savefig(save_plot_to+'M4_fig_12.svg',format='svg',bbox_inches='tight',pad_inches = 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}