{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "# set up imports\n",
    "import pandas as pd, numpy as np, seaborn as sns\n",
    "from tabulate import tabulate\n",
    "import statsmodels\n",
    "import arch\n",
    "import matplotlib\n",
    "matplotlib.use('qt5agg')\n",
    "\n",
    "# configure plot style\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['mathtext.fontset'] = 'stix'\n",
    "plt.rcParams['font.family'] = 'STIXGeneral'\n",
    "plt.rcParams[\"figure.figsize\"] = (9.5,4.15)\n",
    "plt.rcParams['figure.constrained_layout.use'] = False\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['savefig.dpi'] = 100\n",
    "plt.rcParams['lines.linewidth'] = 0.8\n",
    "save_plot_to =  r'C:\\\\Users\\joche\\OneDrive\\03 TUM - TUM-BWL\\Semester 8\\01 Bachelorarbeit\\04 Results\\Plots/'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# load data and crop to start_date:end_date\n",
    "start_date = '2015-08-07'\n",
    "end_date = '2020-06-26'\n",
    "idx = pd.date_range(start_date, end_date)\n",
    "index_name = 'date'\n",
    "mydateparser = lambda x: pd.datetime.strptime(str(x), '%Y-%m-%d %H:%M:%S')\n",
    "mydateparser1 = lambda x: pd.datetime.strptime(str(x), '%Y-%m-%d')\n",
    "\n",
    "# btc\n",
    "btc = pd.read_excel('Data/BTC_closing.xlsx', parse_dates=[0], index_col=0, date_parser=mydateparser)[start_date:end_date]\n",
    "btc.index.name = index_name\n",
    "btc.columns = ['btc']\n",
    "\n",
    "# usd_eur\n",
    "usd_eur = pd.read_excel('Data/DEXUSEU.xls', parse_dates=[0], index_col=0, skiprows=10, date_parser=mydateparser)[start_date:end_date]\n",
    "usd_eur.index.name = index_name\n",
    "usd_eur.columns = ['usd_eur']\n",
    "usd_eur = usd_eur.loc[(usd_eur!=0).any(1)]\n",
    "\n",
    "# tot_btc: only weekly data - missing values interpolated\n",
    "tot_btc = pd.read_csv('Data/total-bitcoins.txt', index_col=0)[start_date:end_date]\n",
    "tot_btc.index = pd.DatetimeIndex(tot_btc.index, normalize=True).normalize()\n",
    "tot_btc.index.name = index_name\n",
    "tot_btc = tot_btc.reindex(idx, fill_value=None)\n",
    "tot_btc.interpolate(method='time', inplace=True, limit_direction='both')\n",
    "tot_btc.columns = ['tot_btc']\n",
    "\n",
    "# hs_rate: only weekly data - missing values interpolated\n",
    "hs_rate = pd.read_csv('Data/hash-rate.txt', index_col=0)[start_date:end_date]\n",
    "hs_rate.index = pd.DatetimeIndex(hs_rate.index, normalize=True).normalize()\n",
    "hs_rate.index.name = index_name\n",
    "hs_rate = hs_rate.reindex(idx, fill_value=None)\n",
    "hs_rate.interpolate(method='time', inplace=True, limit_direction='both')\n",
    "hs_rate.columns = ['hs_rate']\n",
    "\n",
    "# eth\n",
    "eth = pd.read_excel('Data/ETH.xlsx', parse_dates=[0], index_col=0, date_parser=mydateparser, usecols='A,E')[start_date:end_date]\n",
    "eth.index.name = index_name\n",
    "eth.columns = ['eth']\n",
    "\n",
    "# ggl_trends: only weekly data - missing values interpolated\n",
    "# ggl_trends: weighted average of 15 countries\n",
    "ggl_trends = pd.read_csv('Data/googletrends.txt', index_col=0)[start_date:end_date]\n",
    "ggl_trends.index = pd.DatetimeIndex(ggl_trends.index, normalize=True).normalize()\n",
    "ggl_trends.index.name = index_name\n",
    "ggl_trends = ggl_trends.reindex(idx, fill_value=None)\n",
    "ggl_trends.interpolate(method='time', inplace=True, limit_direction='both')\n",
    "ggl_trends['btc-average'] = np.round(ggl_trends.sum(axis=1)/15)\n",
    "ggl_trends = ggl_trends[['btc-average']].copy()\n",
    "ggl_trends.columns = ['ggl_trends']\n",
    "\n",
    "# wiki_views: sum of 99 countries\n",
    "wiki_views = pd.read_excel('Data/wikipedia.xlsx', parse_dates=[0], index_col=0, date_parser=mydateparser1)[start_date:end_date]\n",
    "wiki_views.index = pd.DatetimeIndex(wiki_views.index, normalize=True).normalize()\n",
    "wiki_views.index.name = index_name\n",
    "wiki_views['wiki-total'] = np.round(wiki_views.sum(axis=1))\n",
    "wiki_views = wiki_views[['wiki-total']].copy()\n",
    "wiki_views.columns = ['wiki_views']\n",
    "\n",
    "# wti_oil\n",
    "oil_wti = pd.read_excel('Data/DCOILWTICO.xls', parse_dates=[0], index_col=0, skiprows=10, date_parser=mydateparser)[start_date:end_date]\n",
    "oil_wti.index = pd.DatetimeIndex(oil_wti.index, normalize=True).normalize()\n",
    "oil_wti.index.name = index_name\n",
    "oil_wti.columns = ['oil_wti']\n",
    "oil_wti = oil_wti.loc[(oil_wti!=0).any(1)]\n",
    "\n",
    "# gold\n",
    "gold = pd.read_excel('Data/GOLDAMGBD228NLBM10AM.xls', parse_dates=[0], index_col=0, skiprows=10, date_parser=mydateparser)[start_date:end_date]\n",
    "gold.index = pd.DatetimeIndex(gold.index, normalize=True).normalize()\n",
    "gold.index.name = index_name\n",
    "gold.columns = ['gold']\n",
    "gold = gold.loc[(gold!=0).any(1)]\n",
    "\n",
    "# sp500\n",
    "sp500 = pd.read_excel('Data/SP500.xls', parse_dates=[0], index_col=0, skiprows=10, date_parser=mydateparser)[start_date:end_date]\n",
    "sp500.index = pd.DatetimeIndex(sp500.index, normalize=True).normalize()\n",
    "sp500.index.name = index_name\n",
    "sp500.columns = ['sp500']\n",
    "sp500 = sp500.loc[(sp500!=0).any(1)]\n",
    "\n",
    "# sse \n",
    "sse = pd.read_excel('Data/SSEcomposite.xlsx', parse_dates=[0], index_col=0, header=0, date_parser=mydateparser)[start_date:end_date]\n",
    "sse.index = pd.DatetimeIndex(sse.index, normalize=True).normalize()\n",
    "sse.index.name = index_name\n",
    "sse = sse[['Zuletzt']].copy()\n",
    "sse.columns = ['sse']\n",
    "\n",
    "# ffd_rate\n",
    "ffd_rate = pd.read_excel('Data/DFF.xls', parse_dates=[0], index_col=0, skiprows=10, date_parser=mydateparser)[start_date:end_date]\n",
    "ffd_rate.index = pd.DatetimeIndex(ffd_rate.index, normalize=True).normalize()\n",
    "ffd_rate.index.name = index_name\n",
    "ffd_rate.columns = ['ffd_rate']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# merge data to one df (inner join)\n",
    "from functools import reduce\n",
    "temp = [btc,tot_btc,hs_rate,eth,ggl_trends,wiki_views,usd_eur,oil_wti,\n",
    "              gold,sp500,sse,ffd_rate]\n",
    "df = reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True), temp)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# split data in pre and post bubble 2018\n",
    "pre_bubble_end_date = '2017-12-01'\n",
    "post_bubble_start_date = '2018-02-01'\n",
    "\n",
    "# pre bubble df\n",
    "pre_df = df[:pre_bubble_end_date]\n",
    "\n",
    "# post bubble df\n",
    "post_df = df[post_bubble_start_date:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save btc_new.txt dataset with 1112 observations for use in M1-M3\n",
    "btc_cropped = df['btc']\n",
    "btc_cropped.to_csv(path_or_buf=save_plot_to+'btc_new.txt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "outputs": [],
   "source": [
    "### DEFINE FUNCTIONS ###\n",
    "\n",
    "# define overview-printing function of dataframe\n",
    "def dates_overview(dataframe):\n",
    "    print('start_date:\\t', dataframe.index[0])\n",
    "    print('end_date:\\t', dataframe.index[-1])\n",
    "    print('len df:\\t\\t', len(dataframe))\n",
    "    \n",
    "# define plotting function of dataframe\n",
    "import matplotlib.dates as mdates\n",
    "def plot_df(dataframe):\n",
    "    fig_i, axs = plt.subplots(4,3, figsize=(9.5,4.15))\n",
    "    for i, ax in enumerate(axs.flatten()):\n",
    "        data = dataframe[dataframe.columns[i]]\n",
    "        ax.plot(data, color='black')\n",
    "        ax.set_title(dataframe.columns[i])\n",
    "        ax.xaxis.set_ticks_position('none')\n",
    "        ax.yaxis.set_ticks_position('none')\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m\"))\n",
    "        ax.xaxis.set_minor_formatter(mdates.DateFormatter(\"%Y-%m\"))\n",
    "        #ax.spines['top'].set_alpha(0)\n",
    "        ax.tick_params(labelsize=8)\n",
    "    plt.tight_layout()\n",
    "    return fig_i\n",
    "\n",
    "# define log-taking and relabeling function\n",
    "def log_of_df(dataframe):\n",
    "    df_log = np.log(dataframe)\n",
    "    new_cols = list()\n",
    "    for i in df_log.columns:\n",
    "        new_cols.append(i+'_log')\n",
    "    df_log.columns = new_cols\n",
    "    # fill na value of negative oil price on 2020-04-20 with 0\n",
    "    df_log.fillna(value=0, inplace=True)\n",
    "    return df_log\n",
    "\n",
    "# define correlation-table generating function of dataframe\n",
    "def corr_table_aslatex_of_df(dataframe):\n",
    "    corr = dataframe.corr().round(3)\n",
    "    print(tabulate(corr, headers=corr.columns, showindex=True, tablefmt=\"latex\"))\n",
    "\n",
    "# define heatmap-generating function of dataframe\n",
    "def heatmap_corr_of_df(dataframe):\n",
    "    corr = dataframe.corr()\n",
    "    # more html colors here: https://www.w3schools.com/colors/colors_names.asp\n",
    "    # pal = sns.light_palette('lightgrey', as_cmap=True)\n",
    "    ax = sns.heatmap(corr, xticklabels=corr.columns.values, yticklabels=corr.columns.values, annot=True, \n",
    "            annot_kws={'size':7}, vmin=-1, center=0, vmax=1, cmap=\"YlGnBu\")\n",
    "    bottom, top = ax.get_ylim()\n",
    "    ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "    fig_3 = plt.gcf()\n",
    "    fig_3.set_size_inches(9.5,4.15)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=7.5, labelbottom = False, bottom=False, top = False, labeltop=True)\n",
    "    plt.xticks(fontsize=7.5, rotation=1)\n",
    "    plt.yticks(fontsize=7.5)\n",
    "    \n",
    "# define train/test-splitting function of dataframe\n",
    "def split_traintest_df(dataframe):\n",
    "    train_size = int(len(dataframe) * 0.8)\n",
    "    df_train, df_test = dataframe[0:train_size], dataframe[train_size:]\n",
    "    # create dataframe for printout\n",
    "    data = {'Dataframe': ['dataframe', 'df_train', 'df_test'],\n",
    "            'date_start': [dataframe.index[0], df_train.index[0], df_test.index[0]],\n",
    "            'date_end': [dataframe.index[-1], df_train.index[-1], df_test.index[-1]],\n",
    "            'nobs': [len(dataframe), len(df_train), len(df_test)]}\n",
    "    df_print = pd.DataFrame(data, columns=['Dataframe','date_start','date_end','nobs'])\n",
    "    print(df_print)\n",
    "    return df_train, df_test\n",
    "\n",
    "# define adf- and pp-testing of dataframe with latex-printout on/off\n",
    "from arch.unitroot import ADF, PhillipsPerron\n",
    "def stationarity_tests(dataframe, latex):\n",
    "    for col in dataframe:\n",
    "        adf = ADF(dataframe[col])\n",
    "        pp = PhillipsPerron(dataframe[col])\n",
    "        if latex is False:\n",
    "            # write summary as plain text to std.out\n",
    "            print('Timeseries:\\t',col,'\\n',\n",
    "                  adf.summary(),'\\n\\n',pp.summary(),'\\n\\n\\n')\n",
    "        else:\n",
    "            # write summary as latex to file\n",
    "            with open(save_plot_to + 'Stationarity_Tests_LaTeX.txt', 'a') as myfile:\n",
    "                myfile.write('Timeseries:\\t'+col+'\\n'\n",
    "                         +adf.summary().as_latex()+'\\n\\n'\n",
    "                         +pp.summary().as_latex()+'\\n\\n\\n')\n",
    "\n",
    "# define first difference-taking function of dataframe\n",
    "def diff_of_df(dataframe):\n",
    "    df_train_log_diff = df_train_log.diff()\n",
    "    # relabel columns\n",
    "    new_cols = list()\n",
    "    for i in df_train_log_diff.columns:\n",
    "        new_cols.append(i+'_diff')\n",
    "    df_train_log_diff.columns = new_cols\n",
    "    return df_train_log_diff\n",
    "\n",
    "# define stationaritiy table-generating function of a dataframe filepath\n",
    "def stationarity_table_aslatex_from_df(filepath):\n",
    "    # read in dataframe from .txt file\n",
    "    stationarity_dataframe = pd.read_csv(save_plot_to+filepath, delimiter='\\s+', header=0)\n",
    "    # print dataframe as latex output\n",
    "    print(tabulate(stationarity_dataframe, headers=stationarity_dataframe.columns, showindex=False, tablefmt=\"latex\"))\n",
    "\n",
    "# define granger causality test performing function of a dataframe\n",
    "# code taken from: https://stackoverflow.com/questions/58005681/is-it-possible-to-run-a-vector-autoregression-analysis-on-a-large-gdp-data-with\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "maxlag=15\n",
    "test = 'ssr-chi2test'\n",
    "def grangers_causality_matrix(X_train, variables, test = 'ssr_chi2test', verbose=False):\n",
    "    dataset = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n",
    "    for c in dataset.columns:\n",
    "        for r in dataset.index:\n",
    "            test_result = grangercausalitytests(X_train[[r,c]], maxlag=maxlag, verbose=False)\n",
    "            p_values = [round(test_result[i+1][0][test][1],4) for i in range(maxlag)]\n",
    "            if verbose: print(f'Y = {r}, X = {c}, P Values = {p_values}')\n",
    "            min_p_value = np.min(p_values)\n",
    "            dataset.loc[r,c] = min_p_value\n",
    "    dataset.columns = [var + '_x' for var in variables]\n",
    "    dataset.index = [var + '_y' for var in variables]\n",
    "    return dataset\n",
    "\n",
    "# define granger causality table generating function from dataframe\n",
    "def grangercausality_table_aslatex_of_df(dataframe):\n",
    "    print(tabulate(dataframe, headers=dataframe.columns, showindex=True, tablefmt=\"latex\"))\n",
    "\n",
    "# define grangercausality heatmap generating function of a dataframe\n",
    "def grangercausality_heatmap_of_df(dataframe):\n",
    "    ax = sns.heatmap(dataframe, xticklabels=dataframe.columns.values, yticklabels=dataframe.index.values,\n",
    "                 annot=True, annot_kws={'size':7}, vmin=0, vmax=1, cmap=\"YlGnBu_r\")\n",
    "    bottom, top = ax.get_ylim()\n",
    "    ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(9.5,4.15)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=7, labelbottom = False, bottom=False, top = False, labeltop=True)\n",
    "    plt.xticks(fontsize=7, rotation=45)\n",
    "    plt.yticks(fontsize=7)\n",
    "    plt.show()\n",
    "\n",
    "# define VAR-order selecting function of dataframe\n",
    "from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "def select_VAR_order(dataframe):\n",
    "    var_model = VAR(endog=dataframe)\n",
    "    var_order_res = var_model.select_order(15).summary()\n",
    "    res_as_html = var_order_res.as_html()\n",
    "    df_var_order = pd.read_html(res_as_html, header=0, index_col=0)[0]\n",
    "    return df_var_order\n",
    "\n",
    "# define VAR-order latex-table-generating function of a var_order\n",
    "def var_order_aslatex_of_order(var_order):\n",
    "    print(tabulate(var_order, headers=var_order.columns, showindex=True, tablefmt=\"latex\"))\n",
    "\n",
    "# define VAR-model-fitting function of a dataframe and a lag order\n",
    "def fit_VAR_model_of_df(dataframe, order):\n",
    "    var_model = VAR(endog=dataframe)\n",
    "    var_fitted = var_model.fit(maxlags = order)\n",
    "    return var_fitted\n",
    "\n",
    "\n",
    "# transform var_fitted into data frame\n",
    "# code taken from: https://stackoverflow.com/questions/51734180/converting-statsmodels-summary-object-to-pandas-dataframe\n",
    "def results_summary_to_dataframe(results):\n",
    "    '''take the result of an statsmodel results table and transforms it into a dataframe'''\n",
    "    pvals = results.pvalues\n",
    "    coeff = results.params\n",
    "    conf_lower = results.conf_int()[0]\n",
    "    conf_higher = results.conf_int()[1]\n",
    "\n",
    "    results_df = pd.DataFrame({\"pvals\":pvals,\n",
    "                               \"coeff\":coeff,\n",
    "                               \"conf_lower\":conf_lower,\n",
    "                               \"conf_higher\":conf_higher\n",
    "                                })\n",
    "    #Reordering\n",
    "    results_df = results_df[[\"coeff\",\"pvals\",\"conf_lower\",\"conf_higher\"]]\n",
    "    return results_df\n",
    "\n",
    "# define dataframe generating function from .txt file at filepath\n",
    "def var_coeffs_aslatex_from_txt(filepath):\n",
    "    # read in df from textfile\n",
    "    var_btc_coefficients = pd.read_csv(save_plot_to+filepath, delimiter='\\s+', header=0)\n",
    "    var_btc_coefficients.set_index(var_btc_coefficients.columns[0], inplace=True)\n",
    "    # generate latex output\n",
    "    print(tabulate(var_btc_coefficients.round(4), headers=var_btc_coefficients.columns, showindex=True, tablefmt=\"latex\"))\n",
    "\n",
    "# define rolling forecast function\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "def var_rolling_forecast(dataframe, lag_order):\n",
    "    # create train and test dataframes\n",
    "    train_size = int(len(dataframe) * 0.8)\n",
    "    dataframe_train, dataframe_test = dataframe[0:train_size], dataframe[train_size:]\n",
    "    index = len(dataframe_train) - 1\n",
    "    # initialize lists\n",
    "    pred_val_btc = list()\n",
    "    lo_conf_int_btc = list()\n",
    "    up_conf_int_btc = list()\n",
    "    ausreisser_ctr = 0\n",
    "    # rolling forecast\n",
    "    for i in range(len(dataframe_test)):\n",
    "        # input data, dependent on i\n",
    "        input_data = log_of_df(dataframe).diff().values[lag_order+1:index + i]\n",
    "        #input_data = np.log(dataframe).diff().values[lag_order+1:index + i]\n",
    "        # fit model and predict 1 step\n",
    "        dataframe_var = VAR(endog=input_data)\n",
    "        dataframe_var_fitted = dataframe_var.fit(maxlags=lag_order)\n",
    "        dataframe_var_result = dataframe_var_fitted.forecast_interval(y=input_data, steps=1, alpha=0.05)\n",
    "        # obtain absolute (inversed) btc values for mean prediction, upper- and lower confidence interval\n",
    "        yhat_btc = np.exp(dataframe_var_result[0][0][0] + np.log(dataframe).iloc[:,0][index+i])\n",
    "        lo_conf_btc_val = np.exp(dataframe_var_result[1][0][0] + np.log(dataframe).iloc[:,0][index+i])\n",
    "        up_conf_btc_val = np.exp(dataframe_var_result[2][0][0] + np.log(dataframe).iloc[:,0][index+i])\n",
    "        if (dataframe.iloc[:,0][index+i+1] > up_conf_btc_val) or (dataframe.iloc[:,0][index+i+1] < lo_conf_btc_val):\n",
    "            ausreisser_ctr += 1\n",
    "        pred_val_btc.append(yhat_btc)\n",
    "        lo_conf_int_btc.append(lo_conf_btc_val)\n",
    "        up_conf_int_btc.append(up_conf_btc_val)\n",
    "    # return [0]: mse, [1]: ausreisser ctr, [2]: predictions, [3]: lo_confint, [4]: up_confint\n",
    "    return (np.sqrt(mse(pred_val_btc, dataframe_test.iloc[:,0].values)), ausreisser_ctr,\n",
    "            pred_val_btc, lo_conf_int_btc, up_conf_int_btc)\n",
    "\n",
    "# define series-generating and plotting function for dataframe_var_pred_result\n",
    "def plot_var_pred_result(dataframe, dataframe_train, dataframe_test, dataframe_var_pred_result):\n",
    "    # make series for plotting pred. vs. actual\n",
    "    index_pred = np.arange(len(dataframe_train) + 1, len(dataframe) + 1)\n",
    "    pred_val_btc_series = pd.Series(dataframe_var_pred_result[2], index=index_pred)\n",
    "    test_series_btc = pd.Series(dataframe_test.iloc[:,0].values, index=index_pred)\n",
    "    lo_conf_int_btc_series = pd.Series(dataframe_var_pred_result[3], index=index_pred)\n",
    "    up_conf_int_btc_series = pd.Series(dataframe_var_pred_result[4], index=index_pred)\n",
    "    # create plot\n",
    "    fig = plt.plot(dataframe_train.iloc[:,0].values, label= r'$X_T$', color='black')\n",
    "    plt.plot(test_series_btc, label= r'$X_{T+h}$', color='green')\n",
    "    plt.plot(pred_val_btc_series, label= r'$\\hat{X}_{T+h}$', color= 'red')\n",
    "    plt.fill_between(lo_conf_int_btc_series.index, lo_conf_int_btc_series, up_conf_int_btc_series, color='k', alpha=0.1)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title('Einschrittprognose btc VAR(5)')\n",
    "    return fig\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# overview of df\n",
    "dates_overview(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# plot df\n",
    "fig_1 = plot_df(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "C:\\Users\\joche\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: RuntimeWarning: invalid value encountered in log\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# take log of df\n",
    "df_log = log_of_df(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# plot log time series\n",
    "fig_2 = plot_df(df_log)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# correlation matrix of df\n",
    "corr_table_aslatex_of_df(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# correlation heatmap of df\n",
    "heatmap_corr_of_df(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# split df in train/test\n",
    "df_train, df_test = split_traintest_df(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# take log of df_train\n",
    "df_train_log = log_of_df(df_train)\n",
    "df_train_log"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "# plot df_train_log\n",
    "fig_4 = plot_df(df_train_log)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "# adf- and pp-testing of df_train_log - no latex printout\n",
    "stationarity_tests(df_train_log, latex=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# take first difference of df_train_log\n",
    "df_train_log_diff = diff_of_df(df_train_log)\n",
    "df_train_log_diff"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "# plot df_train_log_diff\n",
    "fig_5 = plot_df(df_train_log_diff[1:])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "# adf- and pp-testing of df_train_log_diff - no latex printout\n",
    "stationarity_tests(df_train_log_diff[1:], latex=False)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "# plot stationarity table of df\n",
    "stationarity_table_aslatex_from_df('M4_stationarity tests.txt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "# perform granger causality test each on each based on df_train_log_diff\n",
    "granger_causality_matrix = grangers_causality_matrix(df_train_log_diff[1:], variables = df_train_log_diff.columns)\n",
    "granger_causality_matrix.round(4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "# generate granger causality table as latex output\n",
    "grangercausality_table_aslatex_of_df(granger_causality_matrix)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "# generate granger causality heatmap of df_train_log_diff\n",
    "grangercausality_heatmap_of_df(granger_causality_matrix)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# select VAR-order for df_train_log_diff: 5\n",
    "df_var_order = select_VAR_order(df_train_log_diff[1:])\n",
    "df_var_order"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# generate var_order latex output from df_var_order\n",
    "var_order_aslatex_of_order(df_var_order)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# obtain VAR-model for df_train_log_diff and lag order = 5\n",
    "df_var = fit_VAR_model_of_df(df_train_log_diff[1:], 5)\n",
    "df_var.summary()\n",
    "# -> Copy summary output and save as .txt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "# read in and plot var_coeffs from textfile: 'VAR_coefficients_df.txt'\n",
    "var_coeffs_aslatex_from_txt('VAR_coefficients_df.txt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "# IRF: impulse response of df_var\n",
    "from statsmodels.tsa.vector_ar.irf import IRAnalysis\n",
    "df_irf = df_var.irf(15)\n",
    "df_irf.plot(orth=False, impulse='ggl_trends_log_diff' , response='btc_log_diff')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "# IRF: cumulative impulse responses of df_var\n",
    "df_irf.plot_cum_effects(orth=True, response='btc_log_diff')\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "C:\\Users\\joche\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: RuntimeWarning: invalid value encountered in log\nC:\\Users\\joche\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:207: RuntimeWarning: invalid value encountered in log\nC:\\Users\\joche\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:208: RuntimeWarning: invalid value encountered in log\nC:\\Users\\joche\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:209: RuntimeWarning: invalid value encountered in log\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "mse:\t\t 507.96568585996687\nausreisser:\t 17\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# rolling forecast: obtain mse, ausreisser, predictions, lo-& up confint for df with lag order = 5\n",
    "df_var_pred_result = var_rolling_forecast(df, 5)\n",
    "\n",
    "# printout results\n",
    "print('mse:\\t\\t', df_var_pred_result[0])\n",
    "print('ausreisser:\\t', df_var_pred_result[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "outputs": [],
   "source": [
    "# rolling forecast: create series and plot them for df_var_pred_result\n",
    "fig_6 = plot_var_pred_result(df,df_train,df_test,df_var_pred_result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "# \n",
    "from statsmodels.tsa.vector_ar.vecm import VECM\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}